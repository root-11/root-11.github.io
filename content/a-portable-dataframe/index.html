<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="../../style.css" rel="stylesheet" type="text/css">
    <title>tablite a portable table for python</title>
</head>
<body>
<br>
<div class = "center">
    <a href="../../index.html" style="text-decoration: none;"><b>BJORN MADSEN'S WEBSITE</b><br></a>
    <hr id="hrid"/>
    <div style="text-align: center; display: inline-block; width: 100%;">
        <a class="title" href="../../about.html">ABOUT</a> &nbsp;
        <a class="title" href="../../contact.html">EMAIL</a> &nbsp;
        <a class="title" href="https://paypal.me/BjornMadsen">DONATE</a>
    </div>
</div>
    <br><br>
    <div style="margin-bottom: 3ch;text-transform: none;"></div>

<div style="margin-bottom: 3ch;text-transform: none;">2020-07-01</div><div class="heading">tablite a portable table for python</div><hr/>    
<p>After cleaning data in the field of logistics, parsing csv files, excel files
and scrubbing <code>utf-8</code>, <code>utf-16</code> and <code>windows1250</code> data for a decade, I thought
it was overdue to synthesize my experiences into a compact clean python package.</p>
<p>However before I start, I want to highlight <strong><em>why</em></strong>. </p>
<p>First of all we're all tired of reinventing the wheel when we need to process 
a bit of data. So we pick some package and try to get it to work:</p>
<p><strong>Pandas</strong> features pythonic slicing and an easy to navigate api, but when using 
Pandas, there's always a huge memory overhead. I never took the time
to investigate, but x10 - x20 times the raw file seems common. In my line of 
work that was often enough not to be able to use pandas at all.</p>
<p><strong>Numpy</strong> is of course close to the metal, and thereby "fast" for data processing.
But fast is really not the problem in dirty data. It's the <code>None</code>'s or <code>Nan</code>'s
that numpy doesn't cope well with, so it would make me jump through various 
hoops to get the data into a state where numpy would be usable. Last but not
least, Numpy has become a language of it's own. I'm sorry to say it but it just 
doesn't seem pythonic anymore.</p>
<p><strong>Arrows</strong> looks like all the great things, but it just isn't ready.</p>
<p><strong>SQLite</strong> is great but just too slow, particularly on disk. Even with WAL off,
inserting 200,000 rows per second into a new table no fun, and after that, doing
repeated incremental queries, become painful. Creating temporary tables that help
to avoid rerunning of queries, always become necessary. A second element is that
SQLite doesn't <code>vacuum</code> itself. A user who does an outer join on 40,000 x 40,000
rows of data, generates 1,600,000,000 temporary records - or 8,000 seconds of inserts - 
which just isn't workable.</p>
<p><strong>Protobuffer</strong> is nice for making the data portable, but the overhead of making 
dirty data fit the form, is just as laborious as implementing all the analytics 
that need to happen afterwards.</p>
<p>So where do we end up? We write some custom built class for the problem at hand and
discover that we've just spent 3 hours doing something that should have taken
20 minutes.</p>
<h3>No more!</h3>
<p>I listened to the old adage that good artists copy, great artist steal, and took
the best of all.</p>
<h3>...Enter: <a href="https://pypi.org/project/tablite">tablite</a></h3>
<p>A python library for tables that does everything you need.</p>
<ul>
<li>it handles all real world datatypes: str,float,bool,int,date,datetime,time.</li>
<li>it behaves like a dict/list, so it can be learned in 10 minutes.</li>
<li>it loads data and determines the data types in one line of code for <code>csv</code>, <code>tsv</code>, <code>txt</code>, <code>xlsx</code>, <code>xls</code>, <code>xlsm</code>, <code>ods</code>.</li>
<li>it detects all date,time and datetime formats covered in dateutil and the turing institutes publicly available tests.</li>
<li>it is portable as JSON in a single function call. </li>
<li>It has all the main stream analytics packaged: SQL join, pandas Groupby, All, Any, Filter</li>
<li>It supports incremental analytics using <code>+=</code> </li>
<li>It has datatype validation</li>
<li>and finally it makes YOU blazingly fast. </li>
</ul>
<p>Pun intended in the last line. Most packages advertise that the code they've 
made is <strong><em>blazingly fast</em></strong>. I think it's bullocks -  A package is as
fast as the underlying algorithms and programming language allows it to be. 
What matters to me is I can solve a data clean up task in about 3 minutes. 
That's fast to me. Not that I can repeatedly run through dirty
data at 4 million rows per second, but then spend all day on making the data fit 
into the framework.</p>
<p>For more details go to: <a href="https://pypi.org/project/tablite">tablite</a> 
<a href="https://travis-ci.org/root-11/tablite"><img alt="Build Status" src="https://travis-ci.org/root-11/tablite.svg?branch=master" /></a> 
<a href="https://codecov.io/gh/root-11/tablite"><img alt="Code coverage" src="https://codecov.io/gh/root-11/tablite/branch/master/graph/badge.svg" /></a></p>

</body>
</html>
