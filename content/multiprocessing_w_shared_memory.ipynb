{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python multiprocessing with shared memory\n",
    "\n",
    "I've fought the past 22 hours with multiprocessing to get a nice context manager to work with a couple of workers on WINDOWS 11.\n",
    "\n",
    "Here are the lessons learned.\n",
    "\n",
    "- The usecases for `Pool.map` is limited to repeated calls on the same data source. I still don't understand how to use it with shared_memory.\n",
    "- [Shared memory arrays](https://docs.python.org/3/library/multiprocessing.shared_memory.html#module-multiprocessing.shared_memory) are very effective if you can control the work done by the workers. F.ex. give task ranges like `Pool.map(f, starmap(tasks))` in this [example](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.starmap)\n",
    "- There is a really good example [here](https://docs.python.org/3/library/multiprocessing.shared_memory.html#multiprocessing.shared_memory.SharedMemory.size).\n",
    "- Shared arrays can outlive the workers, so they can create them and return `shm.name` to the main process for later usage. Just don't close the array.\n",
    "- Multiprocessing on windows isn't hopeless, though there are some parts of the documentation that is omitted on python.org. I don't blame them.\n",
    "- Getting tracebacks from the workers to main was easy thanks to the [`traceback` module](https://docs.python.org/3/library/traceback.html#traceback.print_exc).\n",
    "- Using [`exec`](https://docs.python.org/3/library/functions.html#exec) in the worker allows me to practically do anything.\n",
    "- Tracking and visualising progress using a `task queue` and a `result queue` was helped by [`tqdm`](https://pypi.org/project/tqdm/)\n",
    "- Creating a context manager only required `__enter__` and `__exit__`.\n",
    "- The workers could be spawned at any time using [multiprocessing.Process](https://docs.python.org/3/library/multiprocessing.html#the-process-class).\n",
    "- Moving tasks and results was easy using the [multiprocessing.Queue](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue), just be aware that the exception handler needs [`queue.Empty`](https://docs.python.org/3/library/queue.html#queue.Empty) from the `queue` module - not from multiprocessing.\n",
    "\n",
    "\n",
    "Here is the whole solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import traceback\n",
    "import queue\n",
    "import time\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import shared_memory\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TaskManager(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.tq = multiprocessing.Queue()  # task queue for workers.\n",
    "        self.rq = multiprocessing.Queue()  # result queue for workers.\n",
    "        self.pool = []\n",
    "        self.tasks = {}  # task register for progress tracking\n",
    "        self.results = {}  # result register for progress tracking\n",
    "    \n",
    "    def add(self, task):\n",
    "        if not isinstance(task, dict):\n",
    "            raise TypeError\n",
    "        if not 'id' in task:\n",
    "            raise KeyError(\"expect task to have id, to preserve order\")\n",
    "        task_id = task['id']\n",
    "        if task_id in self.tasks:\n",
    "            raise KeyError(f\"task {task_id} already in use.\")\n",
    "        self.tasks[task_id] = task\n",
    "        self.tq.put(task)\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop()\n",
    "        self.tasks.clear()\n",
    "        self.results.clear()\n",
    "\n",
    "    def start(self):\n",
    "        self.pool = [Worker(name=str(i), tq=self.tq, rq=self.rq) for i in range(2)]\n",
    "        for p in self.pool:\n",
    "            p.start()\n",
    "        while not all(p.is_alive() for p in self.pool):\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    def execute(self):\n",
    "        t = tqdm.tqdm(total=len(self.tasks))\n",
    "        while len(self.tasks) != len(self.results):\n",
    "            try:\n",
    "                result = self.rq.get_nowait()\n",
    "                self.results[result['id']] = result\n",
    "            except queue.Empty:\n",
    "                time.sleep(0.01)\n",
    "            t.update(len(self.results))\n",
    "        t.close()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.tq.put(\"stop\")\n",
    "        while all(p.is_alive() for p in self.pool):\n",
    "            time.sleep(0.01)\n",
    "        print(\"all workers stopped\")\n",
    "        self.pool.clear()\n",
    "  \n",
    "\n",
    "class Worker(multiprocessing.Process):\n",
    "    def __init__(self, name, tq, rq):\n",
    "        super().__init__(group=None, target=self.update, name=name, daemon=False)\n",
    "        self.exit = multiprocessing.Event()\n",
    "        self.tq = tq  # workers task queue\n",
    "        self.rq = rq  # workers result queue\n",
    "        self._quit = False\n",
    "        print(f\"Worker-{self.name}: ready\")\n",
    "                \n",
    "    def update(self):\n",
    "        while True:\n",
    "            try:\n",
    "                task = self.tq.get_nowait()\n",
    "            except queue.Empty:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "            \n",
    "            if task == \"stop\":\n",
    "                print(f\"Worker-{self.name}: stop signal received.\")\n",
    "                self.tq.put_nowait(task)  # this assures that everyone gets it.\n",
    "                self.exit.set()\n",
    "                break\n",
    "            error = \"\"\n",
    "            try:\n",
    "                exec(task['script'])\n",
    "            except Exception as e:\n",
    "                f = io.StringIO()\n",
    "                traceback.print_exc(limit=3, file=f)\n",
    "                f.seek(0)\n",
    "                error = f.read()\n",
    "                f.close()\n",
    "\n",
    "            self.rq.put({'id': task['id'], 'handled by': self.name, 'error': error})            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  # REQUIRED ON WINDOWS.\n",
    "\n",
    "    # Create shared_memory array for workers to access.\n",
    "    a = np.array([1, 1, 2, 3, 5, 8])\n",
    "    shm = shared_memory.SharedMemory(create=True, size=a.nbytes)\n",
    "    b = np.ndarray(a.shape, dtype=a.dtype, buffer=shm.buf)\n",
    "    b[:] = a[:]\n",
    "\n",
    "    task = {\n",
    "        'id':1,\n",
    "        'address': shm.name, 'type': 'shm', \n",
    "        'dtype': a.dtype, 'shape': a.shape, \n",
    "        'script': f\"\"\"# from multiprocssing import shared_memory - is already imported.\n",
    "existing_shm = shared_memory.SharedMemory(name='{shm.name}')\n",
    "c = np.ndarray((6,), dtype=np.{a.dtype}, buffer=existing_shm.buf)\n",
    "c[-1] = 888\n",
    "existing_shm.close()\n",
    "\"\"\"}\n",
    "\n",
    "    tasks = [task]\n",
    "    for i in range(4):\n",
    "        task2 = task.copy()\n",
    "        task2['id'] = 2+i\n",
    "        task2['script'] = f\"\"\"existing_shm = shared_memory.SharedMemory(name='{shm.name}')\n",
    "c = np.ndarray((6,), dtype=np.{a.dtype}, buffer=existing_shm.buf)\n",
    "c[{i}] = 111+{i}  # DIFFERENT!\n",
    "existing_shm.close()\n",
    "time.sleep(0.1)  # Added delay to distribute the few tasks amongst the workers.\n",
    "\"\"\"\n",
    "        tasks.append(task2)\n",
    "    \n",
    "    with TaskManager() as tm:\n",
    "        for task in tasks:\n",
    "            tm.add(task)\n",
    "        tm.execute()\n",
    "\n",
    "        for v in tm.results.items():\n",
    "            print(v)\n",
    "\n",
    "    # Alternative \"low level usage\":\n",
    "    # tm = TaskManager()\n",
    "    # tm.add(task)\n",
    "    # tm.start()\n",
    "    # tm.execute()\n",
    "    # tm.stop()\n",
    "    print(b, f\"assertion that b[-1] == 888 is {b[-1] == 888}\")  \n",
    "    print(b, f\"assertion that b[-1] == 888 is {b[1] == 111}\")  \n",
    "    \n",
    "    shm.close()\n",
    "    shm.unlink()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "Worker-0: ready\n",
    "Worker-1: ready\n",
    "\n",
    "(1, {'id': 1, 'handled by': '0', 'error': ''})\n",
    "(2, {'id': 2, 'handled by': '0', 'error': ''})\n",
    "(3, {'id': 3, 'handled by': '1', 'error': ''})\n",
    "(4, {'id': 4, 'handled by': '0', 'error': ''})\n",
    "(5, {'id': 5, 'handled by': '1', 'error': ''})\n",
    "Worker-0: stop signal received.\n",
    "Worker-1: stop signal received.\n",
    "all workers stopped\n",
    "[111 112 113 114   5 888] assertion that b[-1] == 888 is True \n",
    "[111 112 113 114   5 888] assertion that b[-1] == 888 is False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will probably not run in a notebook, but you can copy-paste it to a script and execute it. Just take notice of the imports (I wrote this on python 3.9.6)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
